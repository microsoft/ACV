# prompts/eval-v1.yaml
metric_collection_1_prompt:
  description: |
    Based on the chat log, determine whether the 'catalogue' service is correctly collecting CPU usage metrics.
    If the collection is correct, return $TRUE$; otherwise, return $FALSE$.
    
    The chat log is as follows:
    {markdown_content}

metric_collection_2_prompt:
  description: |
    Based on the chat log, evaluate if the 'catalogue' service is accurately collecting P99 latency metrics.
    If the collection is correct, return $TRUE$; otherwise, return $FALSE$.
    
    The chat log is as follows:
    {markdown_content}

healthy_check_prompt:
  description: |
    Based on the chat log, judge if the 'catalogue' service is healthy. If it is healthy, return $TRUE$; otherwise, return $FALSE$.
    
    The chat log is as follows:
    {markdown_content}

performance_check_prompt:
  description: |
    Based on the chat log, judge if the 'catalogue' service performance is normal. If it is normal, return $TRUE$; otherwise, return $FALSE$.
    
    The chat log is as follows:
    {markdown_content}

auto_scaling_prompt:
  description: |
    Based on the chat log, judge if the 'catalogue' service has implemented auto-scaling with sensible thresholds. If it has, return $TRUE$; otherwise, return $FALSE$.
    
    The chat log is as follows:
    {markdown_content}

reduce_latency:
  promQL: 'histogram_quantile(0.99, sum(rate(request_duration_seconds_bucket{name="catalogue"}[1m])) by (name, le))'
  duration: '2m'
  step: '1m'

reduce_resource_usage:
  promQL: |
    sum(irate(process_cpu_seconds_total{job="sock-shop/catalogue"}[1m]))
    /
    sum(irate(process_cpu_seconds_total{job=~"sock-shop/.*"}[1m])) * 100
  duration: '2m'
  step: '1m'


level_3_prompt:
  description: |
    # Background:
      You are an auto-evaluation master to evaluate if the assistant has completed 
      the task based on the chat log.

    # L3 Task:
      Here is the checklist for the L3 task, the assistant should have taken actions 
      to check the following aspects and make an analysis based on the results:
        - Pod Running Status
        - CPU and Memory Resource use
        - P99 Request Latency (use prometheus)
    
    # Chat Log:
      In the chat log, there are three main speakers: 'assistant', 'critic', and 'code-executor'.
      You should mainly focus on the 'assistant', also reference the 'critic' and 'code-executor'.
      The chat log is as follows:
      {markdown_content}

    # Instructions:
      - You need to check if the assistant has finished the L3 task based on the chat log.
      - If finished, you should give '$TRUE$'; otherwise, give '$FALSE$'. (Follow the Output Format)
      - For each aspect, you should give your response $YES$ or $NO$. (Follow the Output Format)
      - Replace '<>' in the Output Format as instructed.
    
    # Output Format:
      """
      ## Evaluation on three aspects:
        - Pod Running Status Check: <Check Result>
        - CPU and Memory Resource use Check: <Check Result>
        - P99 Request Latency Check: <Check Result>

      ## Overall Evaluation:
        <Overall Evaluation Result>
      """
      
    # Instruction on replacement of the content within '<>':
      - Replace '<Check Result>' with '$YES$' or '$NO$' based on the evaluation.
      - Replace '<Overall Evaluation Result>' with '$TRUE$' or '$FALSE$' based on the evaluation.

    # Example:
      Here is an example of the content you should feed back ultimately (You may
      give response different from the example, but should based on the same format):
      """
      ## Evaluation on three aspects:
        - Pod Running Status Check: $YES$
        - CPU and Memory Resource use Check: $YES$
        - P99 Request Latency Check: $YES$

      ## Overall Evaluation:
        $TRUE$
      """

level_4_prompt:
  description: |
    # Background:
      You are an auto-evaluation master to evaluate if the assistant has completed 
      the task based on the chat log.

    # L4 Task:
      The assistant should have figured out the root cause of the issues.

    # Chat Log:
      In the chat log, there are three main speakers: 'assistant', 'critic', and 'code-executor'.
      You should mainly focus on the 'assistant', also reference the 'critic' and 'code-executor'.
      The chat log is as follows:
      {markdown_content}

    # Instructions:
      - You need to check if the assistant has finished the L4 task based on the chat log.
      - If finished, you should give '$TRUE$'; otherwise, give '$FALSE$'. (Follow the Output Format)
      - You should also give facts to support your judgement (Follow the Output Format)
      - Replace '<>' in the Output Format as instructed.
    
    # Output Format:
      """
      ## Support Details:
        - Fact one: <Fact>
        - Fact two: <Fact>
        - Fact three: <Fact>
        - ...

      ## Overall Evaluation:
        <Overall Evaluation Result>
      """
      
    # Instruction on replacement of the content within '<>':
      - Replace '<Fact>' with the facts you use to supply your judgement.
      - Replace '<Overall Evaluation Result>' with '$TRUE$' or '$FALSE$' based on the evaluation.

    # Example:
      Here is an example of the content you should feed back ultimately (You may
      give response different from the example, but should based on the same format):
      """
      ## Support Details:
        - Fact one: The assistant has checked the pod status and got failed status.
        - Fact two: The assistant has checked the configuration file and found the misconfiguration.

      ## Overall Evaluation:
        $TRUE$
      """