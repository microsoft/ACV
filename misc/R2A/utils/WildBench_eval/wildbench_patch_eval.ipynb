{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7b3876b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/miniconda3/envs/sae_reasoning/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# Enable auto-reload for modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9cecb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wildbench_eval import compose_eval_item, ARGS, parse_result, placeholder_generation, api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1cf0a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bench_data_full = load_dataset(\"allenai/WildBench\", \"v2\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa8f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"output/reasoning_traces\"\n",
    "# model_name, model_result_path = 'llama-8B_withoutR', os.path.join(data_dir, r\"WildBench_DeepSeek-R1-Distill-Llama-8B_withoutR.jsonl\")\n",
    "# model_name, model_result_path = 'llama-8B_withR', os.path.join(data_dir, r\"WildBench_DeepSeek-R1-Distill-Llama-8B_withR.jsonl\")\n",
    "# model_name, model_result_path = 'qwen-1p5B_withoutR', os.path.join(data_dir, r\"WildBench_DeepSeek-R1-Distill-Qwen-1.5B_withoutR.jsonl\")\n",
    "# model_name, model_result_path = 'qwen-1p5B_withR', os.path.join(data_dir, r\"WildBench_DeepSeek-R1-Distill-Qwen-1.5B_withR.jsonl\")\n",
    "# model_name, model_result_path = 'qwen-7B_withoutR', os.path.join(data_dir, r\"WildBench_DeepSeek-R1-Distill-Qwen-7B_withoutR.jsonl\")\n",
    "# model_name, model_result_path = 'qwen-7B_withR', os.path.join(data_dir, r\"WildBench_DeepSeek-R1-Distill-Qwen-7B_withR.jsonl\")\n",
    "\n",
    "\n",
    "model_name, model_result_path = 'llama-8B_withoutR_finished', os.path.join(data_dir, r\"WildBench_DeepSeek-R1-Distill-Llama-8B_withoutR_finished.jsonl\")\n",
    "# model_name, model_result_path = 'qwen-1p5B_withoutR_finished', os.path.join(data_dir, r\"WildBench_DeepSeek-R1-Distill-Qwen-1.5B_withoutR_finished.jsonl\")\n",
    "# model_name, model_result_path = 'qwen-7B_withoutR_finished', os.path.join(data_dir, r\"WildBench_DeepSeek-R1-Distill-Qwen-7B_withoutR_finished.jsonl\")\n",
    "\n",
    "\n",
    "eval_output_file = model_result_path.replace('.jsonl', '_eval.jsonl')\n",
    "\n",
    "args = ARGS(\n",
    "    mode=\"score\",\n",
    "    eval_template=\"wildbench_eval_template.score.v2.md\",\n",
    "    start_idx=0,\n",
    "    end_idx=-1,\n",
    "    model=\"gpt-4o-20240513\",\n",
    "    max_words_to_eval=-1,\n",
    "    eval_output_file=eval_output_file,\n",
    "    save_interval = 1\n",
    ")\n",
    "\n",
    "openai_args = {\n",
    "    \"prompt\": \"TODO\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stop\": [],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d57340",
   "metadata": {},
   "source": [
    "# build batch requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ef1dbe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4e291c89184a4817', '50e0d808f0a641c8']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensitive_id_fp = r\"output/wildbench_sensitive_ids.txt\"\n",
    "sensitive_ids = []\n",
    "if os.path.exists(sensitive_id_fp):\n",
    "    with open(sensitive_id_fp, \"r\") as f:\n",
    "        sensitive_ids = [line.strip() for line in f.readlines()]\n",
    "sensitive_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fca39d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 727 samples in qwen-7B_withoutR_finished dataset\n"
     ]
    }
   ],
   "source": [
    "target_model_data_full = []\n",
    "with open(model_result_path, 'r') as f:\n",
    "    for line in f:\n",
    "        tmp = json.loads(line)\n",
    "        if '<｜end▁of▁sentence｜>' not in tmp['response']:\n",
    "            continue\n",
    "        end_think_count = len(tmp['response'].split('</think>'))\n",
    "        if end_think_count != 2:\n",
    "            continue\n",
    "        if tmp['unique_id'] in sensitive_ids:\n",
    "            continue\n",
    "        d_tmp = {}\n",
    "        d_tmp['session_id'] = tmp['unique_id']\n",
    "        d_tmp['output'] = [tmp['response'].split('</think>')[-1].strip().replace('<｜end▁of▁sentence｜>', '')]\n",
    "        d_tmp['generator'] = model_name\n",
    "        target_model_data_full.append(d_tmp)\n",
    "print(f\"Total {len(target_model_data_full)} samples in {model_name} dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50cf2a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0023794913314551', '002bc5c909264c8c', '007d897c50e447de', '00c7916a072b4947', '00f46b5fca4c4801']\n",
      "len of l_common_id: 727\n",
      "len of l_common_id: 727\n",
      "len(target_model_data)=727\n",
      "len(ref_model_data)=727\n",
      "Loaded the eval_template from wildbench_eval_template.score.v2.md\n",
      "# examples in candidates: 727; We take 727 for evaluation.\n"
     ]
    }
   ],
   "source": [
    "target_model_data = sorted(target_model_data_full, key=lambda x: x['session_id'])\n",
    "print([e['session_id'] for e in target_model_data][:5])\n",
    "\n",
    "l_common_id = [e['session_id'] for e in target_model_data]\n",
    "print('len of l_common_id:', len(l_common_id))\n",
    "l_common_id = list(set(l_common_id))\n",
    "print('len of l_common_id:', len(l_common_id))\n",
    "bench_data = [e for e in bench_data_full if e['session_id'] in l_common_id]\n",
    "\n",
    "ref_model_data = [None] * len(target_model_data)\n",
    "\n",
    "histories = []\n",
    "last_queries = []\n",
    "checklists = []\n",
    "for t, r in zip(target_model_data, ref_model_data):\n",
    "    id_ = t['session_id']\n",
    "    b = [e for e in bench_data if e['session_id'] == id_][0]\n",
    "    compose_eval_item(b, t, r, histories, last_queries, checklists)\n",
    "print(f\"len(target_model_data)={len(target_model_data)}\")\n",
    "print(f\"len(ref_model_data)={len(ref_model_data)}\")\n",
    "\n",
    "candidates = list(target_model_data)\n",
    "references = list(ref_model_data)    \n",
    "results = placeholder_generation(args, candidates, references, histories, last_queries, checklists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b87b1cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: /mnt/SAE-Reasoning/output/prompt_0_temp_0p6/WildBench_DeepSeek-R1-Distill-Qwen-7B_withoutR_finished_eval.jsonl : 100%|██████████| 727/727 [00:00<00:00, 582920.86it/s]\n"
     ]
    }
   ],
   "source": [
    "l_request = []\n",
    "for ind, item in tqdm(enumerate(results), total=len(results), desc=f\"Evaluating: {args.eval_output_file} \"):\n",
    "    computed = False\n",
    "    if item[\"result\"] != \"N/A\" and item.get(\"error\", \"N/A\") == \"N/A\" and \"parsed_result\" in item:  \n",
    "        results[ind][\"parsed_result\"] = parse_result(results[ind][\"result\"], eval_mode=args.mode) # redo the parsing \n",
    "        results[ind][\"parsed\"] = True if results[ind][\"parsed_result\"] is not None else False\n",
    "        computed = True  \n",
    "        continue\n",
    "        \n",
    "    openai_args[\"prompt\"] = item[\"prompt\"]\n",
    "\n",
    "    request = {\n",
    "        \"custom_id\": f\"run-{model_name}-{item['session_id']}\",\n",
    "        \"method\": \"POST\",\n",
    "        \"url\": \"/chat/completions\",\n",
    "        \"body\": {\n",
    "            \"model\": \"gpt-4o-20240513-batch\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": openai_args['prompt']}\n",
    "            ],\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 1024,\n",
    "        \"stop\": [],\n",
    "        'top_p': 0.95,\n",
    "        'frequency_penalty': 0,\n",
    "        'presence_penalty': 0,\n",
    "        'response_format': {\"type\": \"json_object\"},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    l_request.append(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c675b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = model_result_path.replace(\".jsonl\", \"_eval_requests.jsonl\")\n",
    "\n",
    "with open(input_file_path, \"w\") as f:\n",
    "    for req in l_request:\n",
    "        f.write(json.dumps(req) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af2c717",
   "metadata": {},
   "source": [
    "# process batch request results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de8b3573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "729"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_jsonl(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    return [json.loads(line) for line in lines]\n",
    "\n",
    "results = load_jsonl(model_result_path.replace(\".jsonl\", \"_eval_requests.results.jsonl\"))\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98df2525",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: /mnt/SAE-Reasoning/output/prompt_0_temp_0p6/WildBench_DeepSeek-R1-Distill-Llama-8B_withoutR_finished_eval.jsonl : 100%|██████████| 729/729 [00:00<00:00, 157952.66it/s]\n"
     ]
    }
   ],
   "source": [
    "l_eval_results = []\n",
    "for ind, item in tqdm(enumerate(results), total=len(results), desc=f\"Evaluating: {args.eval_output_file} \"):\n",
    "    parsed_result = parse_result(results[ind][\"response\"]['body']['choices'][0]['message']['content'], eval_mode=args.mode) # redo the parsing\n",
    "    d_tmp = {\n",
    "        \"session_id\": results[ind][\"custom_id\"].split(\"-\")[-1].strip(),\n",
    "        \"result\": results[ind][\"response\"]['body']['choices'][0]['message']['content'],\n",
    "        \"parsed_result\": parsed_result,\n",
    "        \"parsed\": True if parsed_result is not None else False,\n",
    "        \"error\": results[ind][\"error\"],\n",
    "    }\n",
    "    l_eval_results.append(d_tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc222f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.eval_output_file, \"w\") as f:\n",
    "    json.dump(l_eval_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a083513",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae_reasoning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
