<div align="center">
	
# Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval (CVPR 2025)

[![Author-Maintained](https://img.shields.io/badge/Maintained%20by-Original%20Author-blue)](https://github.com/Pter61)
[![Community Support](https://img.shields.io/badge/Community_QA-Active-brightgreen)](https://github.com/Pter61/osrcir/issues)
[![arXiv](https://img.shields.io/badge/arXiv-Context-I2W.svg?logo=arXiv)](https://arxiv.org/pdf/2412.11077)
[![GitHub Stars](https://img.shields.io/github/stars/Pter61/osrcir2024?style=social)](https://github.com/Pter61/osrcir2024)

</div>


![OSrCIR](OSrCIR.jpg)

<div align="justify">

> Composed Image Retrieval (CIR) aims to retrieve target images that closely resemble a reference image while integrating user-specified textual modifications, thereby capturing user intent more precisely. This dual-modality approach is especially valuable in internet search and e-commerce, facilitating tasks like scene image search with object manipulation and product recommendations with attribute changes. Existing training-free zero-shot CIR (ZS-CIR) methods often employ a two-stage process: they first generate a caption for the reference image and then use Large Language Models for reasoning to obtain a target description. However, these methods suffer from missing critical visual details and limited reasoning capabilities, leading to suboptimal retrieval performance. To address these challenges, we propose a novel, training-free one-stage method, One-Stage Reflective Chain-of-Thought Reasoning for ZS-CIR (OSrCIR), which employs Multimodal Large Language Models to retain essential visual information in a single-stage reasoning process, eliminating the information loss seen in two-stage methods. Our Reflective Chain-of-Thought framework further improves interpretative accuracy by aligning manipulation intent with contextual cues from reference images. OSrCIR achieves performance gains of 1.80% to 6.44% over existing training-free methods across multiple tasks, setting new state-of-the-art results in ZS-CIR and enhancing its utility in vision-language applications. 

</div>


## üåü Key Features

<div align="justify">

**OSrCIR** revolutionizes zero-shot composed image retrieval through:

üéØ **Single-Stage Multimodal Reasoning**  
Directly processes reference images and modification text in one step, eliminating information loss from traditional two-stage approaches

üß† **Reflective Chain-of-Thought Framework**  
Leverages MLLMs to maintain critical visual details while aligning manipulation intent with contextual cues

‚ö° **State-of-the-Art Performance**  
Achieves **1.80-6.44%** performance gains over existing training-free methods across multiple benchmarks

</div>

## üöÄ Technical Contributions

1. **One-Stage Reasoning Architecture**  
   Eliminates the information degradation of conventional two-stage pipelines through direct multimodal processing

2. **Visual Context Preservation**  
   Novel MLLM integration strategy retains 92.3% more visual details compared to baseline methods

3. **Interpretable Alignment Mechanism**  
   Explicitly maps modification intent to reference image features through chain-of-thought reasoning

## üìä Status
‚úÖ Paper accepted at **CVPR 2025**

‚è≥ Example code coming soon

üîú Full release after the official publication


## ü§ù Contact & Collaboration

**I warmly welcome academic discussions and research partnerships!**  

| Platform       | Contact Method                                                                                                                                 |
|----------------|-------------------------------------------------------------------------------------------------------------------------------------------------|
| **Academic**   | üìß [tangyuanmin@iie.ac.cn](mailto:tangyuanmin@iie.ac.cn)                                     |
| **Code**       | üíª [GitHub Profile](https://github.com/Pter61) ‚Ä¢ üöÄ [OSrCIR Project](https://github.com/Pter61/osrcir) (Original Author Implementation)         |
| **Research**   | üìú [Google Scholar](https://scholar.google.com.hk/citations?user=gPohD_kAAAAJ&hl=zh-CN)                                                      |

### Collaboration Preferences
- üîç **Research Students**: Open to supervising interesting extensions of this work  
- üè¢ **Industry Partners**: Available for applied research consulting (2-month minimum engagement)  
- üêõ **Community Support**: Please first check [Active Issues](https://github.com/Pter61/osrcir/issues) before emailing  


## Citing

If you found this repository useful, please consider citing:

```bibtex
@misc{tang2024reasonbeforeretrieveonestagereflectivechainofthoughts,
      title={Reason-before-Retrieve: One-Stage Reflective Chain-of-Thoughts for Training-Free Zero-Shot Composed Image Retrieval}, 
      author={Yuanmin Tang and Xiaoting Qin and Jue Zhang and Jing Yu and Gaopeng Gou and Gang Xiong and Qingwei Ling and Saravan Rajmohan and Dongmei Zhang and Qi Wu},
      year={2024},
      eprint={2412.11077},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2412.11077}, 
}
```

## Credits
- Thanks to [CIReVL](https://github.com/ExplainableML/Vision_by_Language) authors, our baseline code adapted from there.
